{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy.sparse import csr_array, triu, save_npz, load_npz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get load tsv file, tab seperated\n",
    "tsv = pd.read_csv(\"data/MINDsmall_train/behaviors.tsv\", sep='\\t')\n",
    "\n",
    "# Replace Nans with empty string (some of the column values are empty, and get filled with Nans)\n",
    "tsv = tsv.replace(np.nan, \" \")\n",
    "\n",
    "# Remove unnecesary columns [added index, and history column]\n",
    "behaviors = tsv.drop(tsv.columns[[0, 2]], axis=1)\n",
    "\n",
    "\n",
    "# Give columns names\n",
    "behaviors = behaviors.set_axis([\"ID\", \"Prior\", \"History\"], axis=1)\n",
    "\n",
    "# Preview DF\n",
    "behaviors.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Remove the U in the ID strings and change to int (assumes all strings start with u)\n",
    "behaviors[\"ID\"] = behaviors[\"ID\"].apply(lambda x: int(x[1:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get all unique user IDs\n",
    "users = behaviors[\"ID\"].unique()\n",
    "\n",
    "# Reindex the IDs\n",
    "behaviors[\"ID\"] = behaviors[\"ID\"].replace(users, np.arange(users.size))\n",
    "\n",
    "# Split strings of impressions in to arrays\n",
    "behaviors[\"Prior\"] = behaviors[\"Prior\"].apply(lambda x: x.split(\" \"))\n",
    "behaviors[\"History\"] = behaviors[\"History\"].apply(lambda x: x.split(\" \"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group rows by ID and combine the values in the rows with the same ID\n",
    "df = behaviors.groupby(behaviors['ID']).aggregate(sum)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview DF\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "\n",
    "def parse_history(history, mode=None):\n",
    "    if mode is not None:\n",
    "        # Get all the items that end in mode, if mode is passed\n",
    "        *history, = filter(lambda x: int(x[-1]) == mode, history)\n",
    "\n",
    "    # use regex to get numbers only\n",
    "    return [int(re.sub(\"[^0-9]\", \"\", h)) for h in history]\n",
    "\n",
    "\n",
    "df[\"clicked\"] = df[\"History\"].apply(lambda x: parse_history(x, mode=0))\n",
    "df[\"viewed\"] = df[\"History\"].apply(lambda x: parse_history(x, mode=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview DF\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def reindex_column(header, items={}, return_items=False):\n",
    "    # For loop builds a dictionary of all articles, and gives them a unique index\n",
    "    for c in header:\n",
    "        for x in c:\n",
    "            if x not in items.keys():\n",
    "                items[x] = len(items.keys())\n",
    "\n",
    "    # Re-index the articles with the new index\n",
    "    *reindex, = header.apply(lambda x: [items[c] for c in x])\n",
    "\n",
    "    if return_items:\n",
    "        reindex = (reindex, items)\n",
    "    return reindex\n",
    "\n",
    "\n",
    "df[\"clicked_re\"], items = reindex_column(df.clicked, return_items=True)\n",
    "df[\"viewed_re\"] = reindex_column(df.viewed, items=items)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preview DF\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n, m = len(users), len(items)\n",
    "adj = np.zeros((n, m), dtype=np.int16)\n",
    "for i, x in df.iterrows():\n",
    "    # Fill in a 1 for the index where an item was clicked\n",
    "    adj[i][x['clicked_re']] = 1\n",
    "\n",
    "# Make Sparse Matrix\n",
    "sa = csr_array(adj)\n",
    "\n",
    "# Matrix Multiplication to get Adjacency Graph (this takes ~50sec on my computer)\n",
    "graph = sa @ sa.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep everything above the main diagnol (so that edges aren't included twice) [took me ~2mins]\n",
    "upper = csr_array(triu(graph, k=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_npz(\"user_user_matrix\", upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper.size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_one = upper.size\n",
    "count_one / ((upper.shape[0] * upper.shape[1]) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "count_two = upper.data[upper.data >= 10].size\n",
    "count_two / ((upper.shape[0] * upper.shape[1]) / 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy.sparse as sp\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "flat_array = upper.data\n",
    "\n",
    "plt.hist(flat_array, bins=range(min(flat_array), 100), edgecolor='black')\n",
    "plt.xlabel('Shared Articles (Count)')\n",
    "plt.ylabel('Frequency')\n",
    "plt.title('Distribution of Shared Articles')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def graph_coverage():\n",
    "\n",
    "    unique_values = range(1, 100)\n",
    "    normalized_frequencies = []\n",
    "\n",
    "    for value in unique_values:\n",
    "        count = upper.data[upper.data >= value].size\n",
    "        freq = count / ((upper.shape[0] * upper.shape[1]) / 2)\n",
    "        normalized_frequencies.append(freq)\n",
    "        print(\n",
    "            f\"Value: {value}, Normalized Frequency: {freq:.4f}, Actual Frequency: {count}\")\n",
    "\n",
    "    # Plot the bar chart\n",
    "    plt.bar(unique_values, normalized_frequencies, edgecolor='black')\n",
    "    plt.title('Normalized Frequencies of Unique Values')\n",
    "    plt.xlabel('Value')\n",
    "    plt.ylabel('Normalized Frequency')\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "graph_coverage()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "upper = load_npz('user_user_matrix.npz')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_rows, filtered_cols = upper.nonzero()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calc_modularity(threshold, user_user_matrix):\n",
    "\n",
    "    mask = user_user_matrix.data >= threshold\n",
    "    # print(mask)\n",
    "    # filtered_rows, filtered_cols = user_user_matrix.nonzero()\n",
    "    filtered_rows_m = filtered_rows[mask]\n",
    "    filtered_cols_m = filtered_cols[mask]\n",
    "    filtered_data = user_user_matrix.data[mask]\n",
    "\n",
    "    percentage = filtered_data.size / ((upper.shape[0] * upper.shape[1]) / 2)\n",
    "    print(percentage)\n",
    "\n",
    "    print(f'making graph with threshold {threshold}')\n",
    "    # Create a graph from the filtered data\n",
    "    G = nx.Graph()\n",
    "    # edge_data = []\n",
    "\n",
    "    # Iterate over the filtered data and add edges\n",
    "    for i, j, w in zip(filtered_rows_m, filtered_cols_m, filtered_data):\n",
    "        if i != j:  # Exclude self-loops\n",
    "            G.add_edge(i, j, weight=w)\n",
    "            # edge_data.append((i, j, w))\n",
    "\n",
    "    # # df_edges = pd.DataFrame(edge_data, columns=['Source', 'Target', 'Weight'])\n",
    "\n",
    "    print('now community detection')\n",
    "\n",
    "    # # compute the best partition\n",
    "    # communities_generator = nx.community.louvain_communities(G)\n",
    "    # partition = {node: i for i, comm in enumerate(\n",
    "    #     communities_generator) for node in comm}\n",
    "    # communities_list = [comm for comm in communities_generator]\n",
    "\n",
    "    # # return communities_list\n",
    "\n",
    "    # modularity_score = nx.community.modularity(G, communities_list)\n",
    "    # coverage, performance = nx.community.partition_quality(G, communities_list)\n",
    "    # # size = sum(1 for community in communities_list if len(community) < 50)\n",
    "    # # small_percentage = size / len(communities_list)\n",
    "\n",
    "    # print(\"modularity:\", modularity_score)\n",
    "    # print(\"coverage:\", coverage)\n",
    "    # print(\"performance:\", performance)\n",
    "    # print(\"percentage\", percentage)\n",
    "\n",
    "    # community_sizes = [len(community)/51281 for community in communities_list]\n",
    "    # print(community_sizes)\n",
    "    # print('total:', sum(community_sizes)/51281)\n",
    "    # print('size percentage:', small_percentage)\n",
    "\n",
    "    # return modularity_score, coverage, performance, percentage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod_score, coverage, performance, percentage = calc_modularity(\n",
    "    threshold=5, user_user_matrix=upper)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mod = []\n",
    "cov = []\n",
    "perf = []\n",
    "perc = []\n",
    "\n",
    "thrs = range(1, 2)\n",
    "for i in thrs:\n",
    "    mod_score, coverage, performance, percentage = calc_modularity(\n",
    "        threshold=i, user_user_matrix=upper[0:10])\n",
    "    mod.append(mod_score)\n",
    "    cov.append(coverage)\n",
    "    perf.append(performance)\n",
    "    perc.append(percentage)\n",
    "\n",
    "\n",
    "plt.plot(thrs, mod, label='y = modularity', color='blue')\n",
    "plt.plot(thrs, cov, label='y = coverage', color='red')\n",
    "plt.plot(thrs, perf, label='y = performance', color='green')\n",
    "plt.plot(thrs, perc, label='y = percentage of users', color='orange')\n",
    "\n",
    "plt.title(\"Threshold vs Metrics\")\n",
    "plt.xlabel(\"threshold\")\n",
    "plt.legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
